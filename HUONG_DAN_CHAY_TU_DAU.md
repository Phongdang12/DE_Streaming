# HÆ°á»›ng Dáº«n Cháº¡y Pipeline Tá»« Äáº§u

## ğŸ“‹ Tá»•ng Quan
Pipeline nÃ y sáº½:
1. Airflow â†’ Kafka: Äáº©y dá»¯ liá»‡u user vÃ o Kafka topic `names_topic`
2. Spark â†’ S3: Spark Structured Streaming consume tá»« Kafka vÃ  ghi Parquet lÃªn S3

---

## ğŸ”§ BÆ°á»›c 1: Khá»Ÿi Äá»™ng Docker Stack

### 1.1. Táº¡o Docker Network (chá»‰ cáº§n cháº¡y 1 láº§n)
```powershell
docker network create docker_streaming
```

### 1.2. Khá»Ÿi Ä‘á»™ng toÃ n bá»™ services
```powershell
cd C:\Users\Admin\Data-Engineering-Streaming-Project
docker compose up -d
```

**Chá» 1-2 phÃºt** Ä‘á»ƒ táº¥t cáº£ containers khá»Ÿi Ä‘á»™ng xong.

### 1.3. Kiá»ƒm tra containers Ä‘ang cháº¡y
```powershell
docker ps
```

Báº¡n sáº½ tháº¥y cÃ¡c containers:
- `airflow_db`, `airflow_webserver`, `airflow_scheduler`
- `kafka_zookeeper`, `kafka_broker_1`, `kafka_broker_2`, `kafka_broker_3`
- `kafka_ui`, `spark_master`, `spark_worker`

---

## ğŸ“¦ BÆ°á»›c 2: Chuáº©n Bá»‹ JARs Cho Spark

**Chá»‰ cáº§n cháº¡y 1 láº§n** (hoáº·c khi báº¡n xÃ³a container `spark_master`):

```powershell
docker exec spark_master bash -c "mkdir -p /opt/spark/jars && curl -L -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && curl -L -o /opt/spark/jars/commons-pool2-2.11.1.jar https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar"
```

Kiá»ƒm tra JARs Ä‘Ã£ táº£i:
```powershell
docker exec spark_master bash -c "ls -lh /opt/spark/jars/*.jar"
```

---

## ğŸ“ BÆ°á»›c 3: Copy File Spark Job VÃ o Container

```powershell
docker cp .\spark_processing.py spark_master:/opt/spark/spark_processing.py
```

Kiá»ƒm tra file Ä‘Ã£ copy:
```powershell
docker exec spark_master bash -c "ls -lh /opt/spark/spark_processing.py"
```

---

## ğŸš€ BÆ°á»›c 4: Cháº¡y Spark Structured Streaming Job

### 4.1. Dá»«ng job cÅ© (náº¿u cÃ³)
```powershell
docker exec spark_master bash -c "pkill -f SparkStructuredStreamingToS3 || true"
```

### 4.2. Cháº¡y Spark job má»›i
```powershell
docker exec spark_master bash -c "nohup /opt/spark/bin/spark-submit --master spark://spark-master:7077 --deploy-mode client --name SparkStructuredStreamingToS3 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 --jars /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/commons-pool2-2.11.1.jar /opt/spark/spark_processing.py > /tmp/spark_s3_job.log 2>&1 &"
```

### 4.3. Kiá»ƒm tra Spark job Ä‘ang cháº¡y
```powershell
docker exec spark_master bash -c "ps aux | grep -E '(spark-submit|spark_processing)' | grep -v grep"
```

Báº¡n sáº½ tháº¥y 2 processes:
- `spark-submit` (Java process)
- `python3 /opt/spark/spark_processing.py`

### 4.4. Xem log Spark job
```powershell
docker exec spark_master bash -c "tail -f /tmp/spark_s3_job.log"
```

**Nháº¥n `Ctrl+C` Ä‘á»ƒ thoÃ¡t** khi Ä‘Ã£ tháº¥y log "Initiating streaming process..." vÃ  "Streaming query started".

**Log thÃ nh cÃ´ng sáº½ cÃ³:**
```
INFO:Spark session initialized successfully
INFO:Streaming dataframe fetched successfully
INFO:Initiating streaming process...
INFO:Streaming query started. Query ID: ...
```

---

## ğŸ“Š BÆ°á»›c 5: Trigger Airflow Äá»ƒ Äáº©y Data VÃ o Kafka

### 5.1. Trigger DAG
```powershell
docker exec airflow_webserver bash -c "airflow dags trigger name_stream_dag"
```

### 5.2. Kiá»ƒm tra Kafka UI
Má»Ÿ trÃ¬nh duyá»‡t: **http://localhost:8888**

- VÃ o **Topics** â†’ `names_topic`
- Xem tab **Overview**: Message Count sáº½ tÄƒng dáº§n
- Xem tab **Messages**: CÃ³ thá»ƒ xem ná»™i dung messages

### 5.3. (TÃ¹y chá»n) Trigger thÃªm nhiá»u láº§n Ä‘á»ƒ cÃ³ nhiá»u data
```powershell
# Trigger 3 láº§n, má»—i láº§n cÃ¡ch nhau 5 giÃ¢y
docker exec airflow_webserver bash -c "airflow dags trigger name_stream_dag"; Start-Sleep -Seconds 5; docker exec airflow_webserver bash -c "airflow dags trigger name_stream_dag"; Start-Sleep -Seconds 5; docker exec airflow_webserver bash -c "airflow dags trigger name_stream_dag"
```

---

## âœ… BÆ°á»›c 6: Kiá»ƒm Tra Data TrÃªn S3

### 6.1. Äá»£i 10-30 giÃ¢y sau khi trigger Airflow
Spark sáº½ xá»­ lÃ½ batch má»—i 10 giÃ¢y (theo trigger Ä‘Ã£ cáº¥u hÃ¬nh).

### 6.2. Kiá»ƒm tra S3 Bucket
1. Má»Ÿ **AWS Console** â†’ **S3**
2. VÃ o bucket `streaming-storages`
3. VÃ o folder `data/`
4. Báº¡n sáº½ tháº¥y cÃ¡c file Parquet:
   - `part-00000-xxxxx-xxxxx.snappy.parquet`
   - `part-00001-xxxxx-xxxxx.snappy.parquet`
   - ...

### 6.3. Kiá»ƒm tra Spark UI (TÃ¹y chá»n)
Má»Ÿ trÃ¬nh duyá»‡t: **http://localhost:8085**

- VÃ o tab **Streaming** Ä‘á»ƒ xem streaming query status
- Xem sá»‘ lÆ°á»£ng batches Ä‘Ã£ xá»­ lÃ½

---

## ğŸ” BÆ°á»›c 7: Kiá»ƒm Tra Logs (Náº¿u CÃ³ Váº¥n Äá»)

### 7.1. Log Spark Job
```powershell
docker exec spark_master bash -c "tail -n 100 /tmp/spark_s3_job.log"
```

### 7.2. Log Airflow
```powershell
docker exec airflow_scheduler bash -c "tail -n 50 /opt/airflow/logs/dag_id=name_stream_dag/*/stream_to_kafka_task/*/*.log"
```

### 7.3. Kiá»ƒm tra Kafka Topic
```powershell
docker exec kafka_broker_1 bash -c "kafka-topics --bootstrap-server localhost:19092 --describe --topic names_topic"
```

---

## ğŸ”„ Khá»Ÿi Äá»™ng Láº¡i Tá»« Äáº§u (Khi Cáº§n)

Náº¿u muá»‘n reset hoÃ n toÃ n:

### 1. Dá»«ng táº¥t cáº£ containers
```powershell
docker compose down
```

### 2. XÃ³a checkpoint vÃ  data trÃªn S3 (náº¿u muá»‘n)
- VÃ o AWS Console â†’ S3 â†’ `streaming-storages`
- XÃ³a folder `checkpoints/` vÃ  `data/`

### 3. Khá»Ÿi Ä‘á»™ng láº¡i tá»« BÆ°á»›c 1

---

## âš ï¸ LÆ°u Ã Quan Trá»ng

1. **Checkpoint trÃªn S3**: 
   - Spark lÆ°u checkpoint táº¡i `s3a://streaming-storages/checkpoints/`
   - Náº¿u muá»‘n Ä‘á»c láº¡i tá»« Ä‘áº§u, pháº£i **xÃ³a folder checkpoint** trÃªn S3

2. **Spark Job pháº£i cháº¡y liÃªn tá»¥c**:
   - Job sáº½ tá»± Ä‘á»™ng consume messages má»›i tá»« Kafka
   - KhÃ´ng cáº§n restart job má»—i khi cÃ³ data má»›i

3. **Trigger Airflow**:
   - Má»—i láº§n trigger sáº½ táº¡o ~12 messages vÃ o Kafka
   - Spark sáº½ tá»± Ä‘á»™ng xá»­ lÃ½ trong vÃ²ng 10 giÃ¢y

4. **Ports**:
   - Airflow UI: http://localhost:8080
   - Kafka UI: http://localhost:8888
   - Spark UI: http://localhost:8085

---

## ğŸ¯ TÃ³m Táº¯t Lá»‡nh Nhanh

```powershell
# 1. Khá»Ÿi Ä‘á»™ng stack
docker compose up -d

# 2. Copy Spark job
docker cp .\spark_processing.py spark_master:/opt/spark/

# 3. Cháº¡y Spark job
docker exec spark_master bash -c "nohup /opt/spark/bin/spark-submit --master spark://spark-master:7077 --deploy-mode client --name SparkStructuredStreamingToS3 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 --jars /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars/commons-pool2-2.11.1.jar /opt/spark/spark_processing.py > /tmp/spark_s3_job.log 2>&1 &"

# 4. Trigger Airflow
docker exec airflow_webserver bash -c "airflow dags trigger name_stream_dag"

# 5. Xem log
docker exec spark_master bash -c "tail -f /tmp/spark_s3_job.log"
```

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸ‰**

